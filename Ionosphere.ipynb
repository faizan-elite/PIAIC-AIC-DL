{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ionosphere.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A6K4_pKiW7B"
      },
      "source": [
        "Ionosphere Data Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRalH7vRibXa"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_DMnwLMiYCW",
        "outputId": "ed1bd3fc-e6e8-4c8b-ceae-31b5ff801e88"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tp\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "print (tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx3X9-Qcisxq"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED4Dwy5qiuQ8"
      },
      "source": [
        "data=pd.read_csv(\"/content/ionosphere_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUY8pZt6wz1r"
      },
      "source": [
        "# Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "KmV5WI2TiyFG",
        "outputId": "bd23c2d9-ba32-4753-dcd8-e784a5abba6d"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature2</th>\n",
              "      <th>feature3</th>\n",
              "      <th>feature4</th>\n",
              "      <th>feature5</th>\n",
              "      <th>feature6</th>\n",
              "      <th>feature7</th>\n",
              "      <th>feature8</th>\n",
              "      <th>feature9</th>\n",
              "      <th>feature10</th>\n",
              "      <th>feature11</th>\n",
              "      <th>feature12</th>\n",
              "      <th>feature13</th>\n",
              "      <th>feature14</th>\n",
              "      <th>feature15</th>\n",
              "      <th>feature16</th>\n",
              "      <th>feature17</th>\n",
              "      <th>feature18</th>\n",
              "      <th>feature19</th>\n",
              "      <th>feature20</th>\n",
              "      <th>feature21</th>\n",
              "      <th>feature22</th>\n",
              "      <th>feature23</th>\n",
              "      <th>feature24</th>\n",
              "      <th>feature25</th>\n",
              "      <th>feature26</th>\n",
              "      <th>feature27</th>\n",
              "      <th>feature28</th>\n",
              "      <th>feature29</th>\n",
              "      <th>feature30</th>\n",
              "      <th>feature31</th>\n",
              "      <th>feature32</th>\n",
              "      <th>feature33</th>\n",
              "      <th>feature34</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.99539</td>\n",
              "      <td>-0.05889</td>\n",
              "      <td>0.85243</td>\n",
              "      <td>0.02306</td>\n",
              "      <td>0.83398</td>\n",
              "      <td>-0.37708</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.03760</td>\n",
              "      <td>0.85243</td>\n",
              "      <td>-0.17755</td>\n",
              "      <td>0.59755</td>\n",
              "      <td>-0.44945</td>\n",
              "      <td>0.60536</td>\n",
              "      <td>-0.38223</td>\n",
              "      <td>0.84356</td>\n",
              "      <td>-0.38542</td>\n",
              "      <td>0.58212</td>\n",
              "      <td>-0.32192</td>\n",
              "      <td>0.56971</td>\n",
              "      <td>-0.29674</td>\n",
              "      <td>0.36946</td>\n",
              "      <td>-0.47357</td>\n",
              "      <td>0.56811</td>\n",
              "      <td>-0.51171</td>\n",
              "      <td>0.41078</td>\n",
              "      <td>-0.46168</td>\n",
              "      <td>0.21266</td>\n",
              "      <td>-0.34090</td>\n",
              "      <td>0.42267</td>\n",
              "      <td>-0.54487</td>\n",
              "      <td>0.18641</td>\n",
              "      <td>-0.45300</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.18829</td>\n",
              "      <td>0.93035</td>\n",
              "      <td>-0.36156</td>\n",
              "      <td>-0.10868</td>\n",
              "      <td>-0.93597</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.04549</td>\n",
              "      <td>0.50874</td>\n",
              "      <td>-0.67743</td>\n",
              "      <td>0.34432</td>\n",
              "      <td>-0.69707</td>\n",
              "      <td>-0.51685</td>\n",
              "      <td>-0.97515</td>\n",
              "      <td>0.05499</td>\n",
              "      <td>-0.62237</td>\n",
              "      <td>0.33109</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>-0.13151</td>\n",
              "      <td>-0.45300</td>\n",
              "      <td>-0.18056</td>\n",
              "      <td>-0.35734</td>\n",
              "      <td>-0.20332</td>\n",
              "      <td>-0.26569</td>\n",
              "      <td>-0.20468</td>\n",
              "      <td>-0.18401</td>\n",
              "      <td>-0.19040</td>\n",
              "      <td>-0.11593</td>\n",
              "      <td>-0.16626</td>\n",
              "      <td>-0.06288</td>\n",
              "      <td>-0.13738</td>\n",
              "      <td>-0.02447</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.03365</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00485</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.12062</td>\n",
              "      <td>0.88965</td>\n",
              "      <td>0.01198</td>\n",
              "      <td>0.73082</td>\n",
              "      <td>0.05346</td>\n",
              "      <td>0.85443</td>\n",
              "      <td>0.00827</td>\n",
              "      <td>0.54591</td>\n",
              "      <td>0.00299</td>\n",
              "      <td>0.83775</td>\n",
              "      <td>-0.13644</td>\n",
              "      <td>0.75535</td>\n",
              "      <td>-0.08540</td>\n",
              "      <td>0.70887</td>\n",
              "      <td>-0.27502</td>\n",
              "      <td>0.43385</td>\n",
              "      <td>-0.12062</td>\n",
              "      <td>0.57528</td>\n",
              "      <td>-0.40220</td>\n",
              "      <td>0.58984</td>\n",
              "      <td>-0.22145</td>\n",
              "      <td>0.43100</td>\n",
              "      <td>-0.17365</td>\n",
              "      <td>0.60436</td>\n",
              "      <td>-0.24180</td>\n",
              "      <td>0.56045</td>\n",
              "      <td>-0.38238</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.45161</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.71216</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>0.14516</td>\n",
              "      <td>0.54094</td>\n",
              "      <td>-0.39330</td>\n",
              "      <td>-1.00000</td>\n",
              "      <td>-0.54467</td>\n",
              "      <td>-0.69975</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.90695</td>\n",
              "      <td>0.51613</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.20099</td>\n",
              "      <td>0.25682</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.32382</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>-0.02401</td>\n",
              "      <td>0.94140</td>\n",
              "      <td>0.06531</td>\n",
              "      <td>0.92106</td>\n",
              "      <td>-0.23255</td>\n",
              "      <td>0.77152</td>\n",
              "      <td>-0.16399</td>\n",
              "      <td>0.52798</td>\n",
              "      <td>-0.20275</td>\n",
              "      <td>0.56409</td>\n",
              "      <td>-0.00712</td>\n",
              "      <td>0.34395</td>\n",
              "      <td>-0.27457</td>\n",
              "      <td>0.52940</td>\n",
              "      <td>-0.21780</td>\n",
              "      <td>0.45107</td>\n",
              "      <td>-0.17813</td>\n",
              "      <td>0.05982</td>\n",
              "      <td>-0.35575</td>\n",
              "      <td>0.02309</td>\n",
              "      <td>-0.52879</td>\n",
              "      <td>0.03286</td>\n",
              "      <td>-0.65158</td>\n",
              "      <td>0.13290</td>\n",
              "      <td>-0.53206</td>\n",
              "      <td>0.02431</td>\n",
              "      <td>-0.62197</td>\n",
              "      <td>-0.05707</td>\n",
              "      <td>-0.59573</td>\n",
              "      <td>-0.04608</td>\n",
              "      <td>-0.65697</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feature1  feature2  feature3  ...  feature33  feature34  label\n",
              "0         1         0   0.99539  ...    0.18641   -0.45300      g\n",
              "1         1         0   1.00000  ...   -0.13738   -0.02447      b\n",
              "2         1         0   1.00000  ...    0.56045   -0.38238      g\n",
              "3         1         0   1.00000  ...   -0.32382    1.00000      b\n",
              "4         1         0   1.00000  ...   -0.04608   -0.65697      g\n",
              "\n",
              "[5 rows x 35 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df3J85xXyz-Y",
        "outputId": "9443d144-317a-4f95-bd5a-0ceacbeaa581"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(351, 35)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGcatYGT9Rav"
      },
      "source": [
        "There are 35 Columns. 34 Columns have features and Last Column is the Label with (g = good and b = bad). 351 Rows of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llYmupq8jDhS",
        "outputId": "aba4ec9b-433b-45e9-febd-0ccd765c0bcd"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 351 entries, 0 to 350\n",
            "Data columns (total 35 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   feature1   351 non-null    int64  \n",
            " 1   feature2   351 non-null    int64  \n",
            " 2   feature3   351 non-null    float64\n",
            " 3   feature4   351 non-null    float64\n",
            " 4   feature5   351 non-null    float64\n",
            " 5   feature6   351 non-null    float64\n",
            " 6   feature7   351 non-null    float64\n",
            " 7   feature8   351 non-null    float64\n",
            " 8   feature9   351 non-null    float64\n",
            " 9   feature10  351 non-null    float64\n",
            " 10  feature11  351 non-null    float64\n",
            " 11  feature12  351 non-null    float64\n",
            " 12  feature13  351 non-null    float64\n",
            " 13  feature14  351 non-null    float64\n",
            " 14  feature15  351 non-null    float64\n",
            " 15  feature16  351 non-null    float64\n",
            " 16  feature17  351 non-null    float64\n",
            " 17  feature18  351 non-null    float64\n",
            " 18  feature19  351 non-null    float64\n",
            " 19  feature20  351 non-null    float64\n",
            " 20  feature21  351 non-null    float64\n",
            " 21  feature22  351 non-null    float64\n",
            " 22  feature23  351 non-null    float64\n",
            " 23  feature24  351 non-null    float64\n",
            " 24  feature25  351 non-null    float64\n",
            " 25  feature26  351 non-null    float64\n",
            " 26  feature27  351 non-null    float64\n",
            " 27  feature28  351 non-null    float64\n",
            " 28  feature29  351 non-null    float64\n",
            " 29  feature30  351 non-null    float64\n",
            " 30  feature31  351 non-null    float64\n",
            " 31  feature32  351 non-null    float64\n",
            " 32  feature33  351 non-null    float64\n",
            " 33  feature34  351 non-null    float64\n",
            " 34  label      351 non-null    object \n",
            "dtypes: float64(32), int64(2), object(1)\n",
            "memory usage: 96.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wybFcmor7i4"
      },
      "source": [
        "All feature column data is in numbers hence no need to vectorize the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16V5KsVSx4oj"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "Jf6rESoHx9xV",
        "outputId": "f4a1a886-4cd7-4cf2-8b4d-cd57644cc68e"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature2</th>\n",
              "      <th>feature3</th>\n",
              "      <th>feature4</th>\n",
              "      <th>feature5</th>\n",
              "      <th>feature6</th>\n",
              "      <th>feature7</th>\n",
              "      <th>feature8</th>\n",
              "      <th>feature9</th>\n",
              "      <th>feature10</th>\n",
              "      <th>feature11</th>\n",
              "      <th>feature12</th>\n",
              "      <th>feature13</th>\n",
              "      <th>feature14</th>\n",
              "      <th>feature15</th>\n",
              "      <th>feature16</th>\n",
              "      <th>feature17</th>\n",
              "      <th>feature18</th>\n",
              "      <th>feature19</th>\n",
              "      <th>feature20</th>\n",
              "      <th>feature21</th>\n",
              "      <th>feature22</th>\n",
              "      <th>feature23</th>\n",
              "      <th>feature24</th>\n",
              "      <th>feature25</th>\n",
              "      <th>feature26</th>\n",
              "      <th>feature27</th>\n",
              "      <th>feature28</th>\n",
              "      <th>feature29</th>\n",
              "      <th>feature30</th>\n",
              "      <th>feature31</th>\n",
              "      <th>feature32</th>\n",
              "      <th>feature33</th>\n",
              "      <th>feature34</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.0</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "      <td>351.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.891738</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.641342</td>\n",
              "      <td>0.044372</td>\n",
              "      <td>0.601068</td>\n",
              "      <td>0.115889</td>\n",
              "      <td>0.550095</td>\n",
              "      <td>0.119360</td>\n",
              "      <td>0.511848</td>\n",
              "      <td>0.181345</td>\n",
              "      <td>0.476183</td>\n",
              "      <td>0.155040</td>\n",
              "      <td>0.400801</td>\n",
              "      <td>0.093414</td>\n",
              "      <td>0.344159</td>\n",
              "      <td>0.071132</td>\n",
              "      <td>0.381949</td>\n",
              "      <td>-0.003617</td>\n",
              "      <td>0.359390</td>\n",
              "      <td>-0.024025</td>\n",
              "      <td>0.336695</td>\n",
              "      <td>0.008296</td>\n",
              "      <td>0.362475</td>\n",
              "      <td>-0.057406</td>\n",
              "      <td>0.396135</td>\n",
              "      <td>-0.071187</td>\n",
              "      <td>0.541641</td>\n",
              "      <td>-0.069538</td>\n",
              "      <td>0.378445</td>\n",
              "      <td>-0.027907</td>\n",
              "      <td>0.352514</td>\n",
              "      <td>-0.003794</td>\n",
              "      <td>0.349364</td>\n",
              "      <td>0.014480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.311155</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.497708</td>\n",
              "      <td>0.441435</td>\n",
              "      <td>0.519862</td>\n",
              "      <td>0.460810</td>\n",
              "      <td>0.492654</td>\n",
              "      <td>0.520750</td>\n",
              "      <td>0.507066</td>\n",
              "      <td>0.483851</td>\n",
              "      <td>0.563496</td>\n",
              "      <td>0.494817</td>\n",
              "      <td>0.622186</td>\n",
              "      <td>0.494873</td>\n",
              "      <td>0.652828</td>\n",
              "      <td>0.458371</td>\n",
              "      <td>0.618020</td>\n",
              "      <td>0.496762</td>\n",
              "      <td>0.626267</td>\n",
              "      <td>0.519076</td>\n",
              "      <td>0.609828</td>\n",
              "      <td>0.518166</td>\n",
              "      <td>0.603767</td>\n",
              "      <td>0.527456</td>\n",
              "      <td>0.578451</td>\n",
              "      <td>0.508495</td>\n",
              "      <td>0.516205</td>\n",
              "      <td>0.550025</td>\n",
              "      <td>0.575886</td>\n",
              "      <td>0.507974</td>\n",
              "      <td>0.571483</td>\n",
              "      <td>0.513574</td>\n",
              "      <td>0.522663</td>\n",
              "      <td>0.468337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.472135</td>\n",
              "      <td>-0.064735</td>\n",
              "      <td>0.412660</td>\n",
              "      <td>-0.024795</td>\n",
              "      <td>0.211310</td>\n",
              "      <td>-0.054840</td>\n",
              "      <td>0.087110</td>\n",
              "      <td>-0.048075</td>\n",
              "      <td>0.021120</td>\n",
              "      <td>-0.065265</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.073725</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.081705</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.225690</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.234670</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.243870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.366885</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.332390</td>\n",
              "      <td>0.286435</td>\n",
              "      <td>-0.443165</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.236885</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.242595</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.165350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.871110</td>\n",
              "      <td>0.016310</td>\n",
              "      <td>0.809200</td>\n",
              "      <td>0.022800</td>\n",
              "      <td>0.728730</td>\n",
              "      <td>0.014710</td>\n",
              "      <td>0.684210</td>\n",
              "      <td>0.018290</td>\n",
              "      <td>0.667980</td>\n",
              "      <td>0.028250</td>\n",
              "      <td>0.644070</td>\n",
              "      <td>0.030270</td>\n",
              "      <td>0.601940</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.590910</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.576190</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.499090</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.531760</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.553890</td>\n",
              "      <td>-0.015050</td>\n",
              "      <td>0.708240</td>\n",
              "      <td>-0.017690</td>\n",
              "      <td>0.496640</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.442770</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.409560</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.194185</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.334655</td>\n",
              "      <td>0.969240</td>\n",
              "      <td>0.445675</td>\n",
              "      <td>0.953240</td>\n",
              "      <td>0.534195</td>\n",
              "      <td>0.957895</td>\n",
              "      <td>0.482375</td>\n",
              "      <td>0.955505</td>\n",
              "      <td>0.374860</td>\n",
              "      <td>0.919330</td>\n",
              "      <td>0.308975</td>\n",
              "      <td>0.935705</td>\n",
              "      <td>0.195285</td>\n",
              "      <td>0.899265</td>\n",
              "      <td>0.134370</td>\n",
              "      <td>0.894865</td>\n",
              "      <td>0.188760</td>\n",
              "      <td>0.911235</td>\n",
              "      <td>0.164630</td>\n",
              "      <td>0.905240</td>\n",
              "      <td>0.156765</td>\n",
              "      <td>0.999945</td>\n",
              "      <td>0.153535</td>\n",
              "      <td>0.883465</td>\n",
              "      <td>0.154075</td>\n",
              "      <td>0.857620</td>\n",
              "      <td>0.200120</td>\n",
              "      <td>0.813765</td>\n",
              "      <td>0.171660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         feature1  feature2    feature3  ...   feature32   feature33   feature34\n",
              "count  351.000000     351.0  351.000000  ...  351.000000  351.000000  351.000000\n",
              "mean     0.891738       0.0    0.641342  ...   -0.003794    0.349364    0.014480\n",
              "std      0.311155       0.0    0.497708  ...    0.513574    0.522663    0.468337\n",
              "min      0.000000       0.0   -1.000000  ...   -1.000000   -1.000000   -1.000000\n",
              "25%      1.000000       0.0    0.472135  ...   -0.242595    0.000000   -0.165350\n",
              "50%      1.000000       0.0    0.871110  ...    0.000000    0.409560    0.000000\n",
              "75%      1.000000       0.0    1.000000  ...    0.200120    0.813765    0.171660\n",
              "max      1.000000       0.0    1.000000  ...    1.000000    1.000000    1.000000\n",
              "\n",
              "[8 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG0abZQmsGjg"
      },
      "source": [
        "Unique Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLXwIjY2bY3Q",
        "outputId": "62a45890-54ee-47cf-8212-c956c63df2a4"
      },
      "source": [
        "for features in data:\n",
        "    print(features)\n",
        "    print(len(data[features].unique()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature1\n",
            "2\n",
            "feature2\n",
            "1\n",
            "feature3\n",
            "219\n",
            "feature4\n",
            "269\n",
            "feature5\n",
            "204\n",
            "feature6\n",
            "259\n",
            "feature7\n",
            "231\n",
            "feature8\n",
            "260\n",
            "feature9\n",
            "244\n",
            "feature10\n",
            "267\n",
            "feature11\n",
            "246\n",
            "feature12\n",
            "269\n",
            "feature13\n",
            "238\n",
            "feature14\n",
            "266\n",
            "feature15\n",
            "234\n",
            "feature16\n",
            "270\n",
            "feature17\n",
            "254\n",
            "feature18\n",
            "280\n",
            "feature19\n",
            "254\n",
            "feature20\n",
            "266\n",
            "feature21\n",
            "248\n",
            "feature22\n",
            "265\n",
            "feature23\n",
            "248\n",
            "feature24\n",
            "264\n",
            "feature25\n",
            "256\n",
            "feature26\n",
            "273\n",
            "feature27\n",
            "256\n",
            "feature28\n",
            "281\n",
            "feature29\n",
            "244\n",
            "feature30\n",
            "266\n",
            "feature31\n",
            "243\n",
            "feature32\n",
            "263\n",
            "feature33\n",
            "245\n",
            "feature34\n",
            "263\n",
            "label\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1clSdk6_9m_u"
      },
      "source": [
        "Feature 2 has 0 value Hence it needs to be dropped. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt5D6fu4sRZD"
      },
      "source": [
        "data.drop(data.columns[1], inplace=True, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0qo4rCb_vr8",
        "outputId": "5d990a71-05c1-4122-ba05-f4a55f31c92f"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(351, 34)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec6CHFHo-QM4"
      },
      "source": [
        "Check Missing Values (If Any)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvRGuvyN-S2B",
        "outputId": "f6511421-5559-4b01-c5da-c0129196ce93"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "feature1     0\n",
              "feature3     0\n",
              "feature4     0\n",
              "feature5     0\n",
              "feature6     0\n",
              "feature7     0\n",
              "feature8     0\n",
              "feature9     0\n",
              "feature10    0\n",
              "feature11    0\n",
              "feature12    0\n",
              "feature13    0\n",
              "feature14    0\n",
              "feature15    0\n",
              "feature16    0\n",
              "feature17    0\n",
              "feature18    0\n",
              "feature19    0\n",
              "feature20    0\n",
              "feature21    0\n",
              "feature22    0\n",
              "feature23    0\n",
              "feature24    0\n",
              "feature25    0\n",
              "feature26    0\n",
              "feature27    0\n",
              "feature28    0\n",
              "feature29    0\n",
              "feature30    0\n",
              "feature31    0\n",
              "feature32    0\n",
              "feature33    0\n",
              "feature34    0\n",
              "label        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGHm6E-5-YI0"
      },
      "source": [
        "No Missing Values in Dataset\n",
        "\n",
        "Change Labels (Good / Bad to 1 / 0) via One Hot Encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qv5aBGv-iqD",
        "outputId": "45fbd2a3-ebd1-4df8-dc31-be4ce3cf59ba"
      },
      "source": [
        "data['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "g    225\n",
              "b    126\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gof99FBL-pCv"
      },
      "source": [
        "data['label'] = data['label'].apply(lambda x: 1 if x == 'g' else 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ3ka5_QAfTd"
      },
      "source": [
        "Normalize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "NkjxV8c42v1F",
        "outputId": "82e1ec45-0e69-44a9-8520-1a550a35429b"
      },
      "source": [
        "mean = data.iloc[:,:-1].mean(axis=0) # taking the mean of \n",
        "data.iloc[:,:-1] -= mean\n",
        "std = data.iloc[:,:-1].std(axis=0)\n",
        "data.iloc[:,:-1] /= std\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature3</th>\n",
              "      <th>feature4</th>\n",
              "      <th>feature5</th>\n",
              "      <th>feature6</th>\n",
              "      <th>feature7</th>\n",
              "      <th>feature8</th>\n",
              "      <th>feature9</th>\n",
              "      <th>feature10</th>\n",
              "      <th>feature11</th>\n",
              "      <th>feature12</th>\n",
              "      <th>feature13</th>\n",
              "      <th>feature14</th>\n",
              "      <th>feature15</th>\n",
              "      <th>feature16</th>\n",
              "      <th>feature17</th>\n",
              "      <th>feature18</th>\n",
              "      <th>feature19</th>\n",
              "      <th>feature20</th>\n",
              "      <th>feature21</th>\n",
              "      <th>feature22</th>\n",
              "      <th>feature23</th>\n",
              "      <th>feature24</th>\n",
              "      <th>feature25</th>\n",
              "      <th>feature26</th>\n",
              "      <th>feature27</th>\n",
              "      <th>feature28</th>\n",
              "      <th>feature29</th>\n",
              "      <th>feature30</th>\n",
              "      <th>feature31</th>\n",
              "      <th>feature32</th>\n",
              "      <th>feature33</th>\n",
              "      <th>feature34</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.347937</td>\n",
              "      <td>0.711357</td>\n",
              "      <td>-0.233923</td>\n",
              "      <td>0.483517</td>\n",
              "      <td>-0.201447</td>\n",
              "      <td>0.576236</td>\n",
              "      <td>-0.953318</td>\n",
              "      <td>0.962700</td>\n",
              "      <td>-0.297086</td>\n",
              "      <td>0.667701</td>\n",
              "      <td>-0.672148</td>\n",
              "      <td>0.316222</td>\n",
              "      <td>-1.096977</td>\n",
              "      <td>0.400107</td>\n",
              "      <td>-0.989074</td>\n",
              "      <td>0.746919</td>\n",
              "      <td>-0.768584</td>\n",
              "      <td>0.355648</td>\n",
              "      <td>-0.573895</td>\n",
              "      <td>0.382099</td>\n",
              "      <td>-0.588684</td>\n",
              "      <td>0.011568</td>\n",
              "      <td>-0.789002</td>\n",
              "      <td>0.297303</td>\n",
              "      <td>-0.866328</td>\n",
              "      <td>-0.253506</td>\n",
              "      <td>-0.712953</td>\n",
              "      <td>-0.287879</td>\n",
              "      <td>-0.616159</td>\n",
              "      <td>0.122762</td>\n",
              "      <td>-1.053550</td>\n",
              "      <td>-0.311776</td>\n",
              "      <td>-0.998170</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.347937</td>\n",
              "      <td>0.720619</td>\n",
              "      <td>-0.527058</td>\n",
              "      <td>0.633404</td>\n",
              "      <td>-1.036108</td>\n",
              "      <td>-1.337197</td>\n",
              "      <td>-2.026559</td>\n",
              "      <td>0.962700</td>\n",
              "      <td>-0.468813</td>\n",
              "      <td>0.057777</td>\n",
              "      <td>-1.682379</td>\n",
              "      <td>-0.090779</td>\n",
              "      <td>-1.597348</td>\n",
              "      <td>-1.318892</td>\n",
              "      <td>-2.282612</td>\n",
              "      <td>-0.529042</td>\n",
              "      <td>-1.245573</td>\n",
              "      <td>-0.045188</td>\n",
              "      <td>-1.880216</td>\n",
              "      <td>-0.767766</td>\n",
              "      <td>-0.890248</td>\n",
              "      <td>-0.899412</td>\n",
              "      <td>-0.568643</td>\n",
              "      <td>-1.036310</td>\n",
              "      <td>-0.382508</td>\n",
              "      <td>-1.445785</td>\n",
              "      <td>-0.208122</td>\n",
              "      <td>-0.987775</td>\n",
              "      <td>-0.173282</td>\n",
              "      <td>-0.907767</td>\n",
              "      <td>-0.115049</td>\n",
              "      <td>-0.931276</td>\n",
              "      <td>-0.083167</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.347937</td>\n",
              "      <td>0.720619</td>\n",
              "      <td>-0.176746</td>\n",
              "      <td>0.767382</td>\n",
              "      <td>-0.240965</td>\n",
              "      <td>0.913227</td>\n",
              "      <td>-0.460836</td>\n",
              "      <td>0.745075</td>\n",
              "      <td>-0.350036</td>\n",
              "      <td>0.451888</td>\n",
              "      <td>-0.205289</td>\n",
              "      <td>0.729089</td>\n",
              "      <td>-0.172052</td>\n",
              "      <td>0.309041</td>\n",
              "      <td>-0.148662</td>\n",
              "      <td>0.737518</td>\n",
              "      <td>-0.267378</td>\n",
              "      <td>0.632255</td>\n",
              "      <td>-0.118240</td>\n",
              "      <td>0.610294</td>\n",
              "      <td>-0.546767</td>\n",
              "      <td>0.118215</td>\n",
              "      <td>-0.119847</td>\n",
              "      <td>0.309698</td>\n",
              "      <td>-0.650967</td>\n",
              "      <td>0.093372</td>\n",
              "      <td>-0.276192</td>\n",
              "      <td>0.091259</td>\n",
              "      <td>-0.286910</td>\n",
              "      <td>0.440689</td>\n",
              "      <td>-0.463431</td>\n",
              "      <td>0.403867</td>\n",
              "      <td>-0.847381</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.347937</td>\n",
              "      <td>0.720619</td>\n",
              "      <td>-1.123568</td>\n",
              "      <td>0.767382</td>\n",
              "      <td>1.918601</td>\n",
              "      <td>0.328963</td>\n",
              "      <td>-2.149516</td>\n",
              "      <td>-1.009432</td>\n",
              "      <td>-0.374796</td>\n",
              "      <td>-0.845050</td>\n",
              "      <td>-0.313329</td>\n",
              "      <td>-0.644182</td>\n",
              "      <td>-0.188763</td>\n",
              "      <td>-2.058980</td>\n",
              "      <td>0.161502</td>\n",
              "      <td>0.257259</td>\n",
              "      <td>-0.784446</td>\n",
              "      <td>-2.170624</td>\n",
              "      <td>-1.003023</td>\n",
              "      <td>-1.699569</td>\n",
              "      <td>1.913874</td>\n",
              "      <td>-0.600356</td>\n",
              "      <td>0.108835</td>\n",
              "      <td>1.043935</td>\n",
              "      <td>1.923594</td>\n",
              "      <td>-0.049420</td>\n",
              "      <td>1.944525</td>\n",
              "      <td>1.079303</td>\n",
              "      <td>-0.340732</td>\n",
              "      <td>-0.167448</td>\n",
              "      <td>1.954525</td>\n",
              "      <td>-1.287987</td>\n",
              "      <td>2.104295</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.347937</td>\n",
              "      <td>0.720619</td>\n",
              "      <td>-0.154908</td>\n",
              "      <td>0.654659</td>\n",
              "      <td>-0.109761</td>\n",
              "      <td>0.752993</td>\n",
              "      <td>-0.675776</td>\n",
              "      <td>0.512107</td>\n",
              "      <td>-0.713723</td>\n",
              "      <td>0.091921</td>\n",
              "      <td>-0.723076</td>\n",
              "      <td>0.262444</td>\n",
              "      <td>-0.203151</td>\n",
              "      <td>-0.000320</td>\n",
              "      <td>-0.754198</td>\n",
              "      <td>0.238586</td>\n",
              "      <td>-0.431159</td>\n",
              "      <td>0.146392</td>\n",
              "      <td>-0.296884</td>\n",
              "      <td>-0.454022</td>\n",
              "      <td>-0.702566</td>\n",
              "      <td>-0.562113</td>\n",
              "      <td>-0.893693</td>\n",
              "      <td>-0.628013</td>\n",
              "      <td>-1.141395</td>\n",
              "      <td>-0.791819</td>\n",
              "      <td>-0.840911</td>\n",
              "      <td>-0.614940</td>\n",
              "      <td>-1.169475</td>\n",
              "      <td>-0.716703</td>\n",
              "      <td>-1.152581</td>\n",
              "      <td>-0.756593</td>\n",
              "      <td>-1.433689</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feature1  feature3  feature4  ...  feature33  feature34  label\n",
              "0  0.347937  0.711357 -0.233923  ...  -0.311776  -0.998170      1\n",
              "1  0.347937  0.720619 -0.527058  ...  -0.931276  -0.083167      0\n",
              "2  0.347937  0.720619 -0.176746  ...   0.403867  -0.847381      1\n",
              "3  0.347937  0.720619 -1.123568  ...  -1.287987   2.104295      0\n",
              "4  0.347937  0.720619 -0.154908  ...  -0.756593  -1.433689      1\n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvSCZjmpBy2v"
      },
      "source": [
        "#shuffle & Split the data 60% (Training) & 40% (Testing)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5Odq-X-sqhJ"
      },
      "source": [
        "training_data = data.sample(frac= 0.6, random_state=125)\n",
        "testing_data = data.drop(training_data.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aBhP3rzsyEN"
      },
      "source": [
        "\n",
        "training_label = training_data.iloc[:,-1]\n",
        "training_data = training_data.iloc[:,0:-1]\n",
        "testing_label = testing_data.iloc[:,-1]\n",
        "testing_data = testing_data.iloc[:,0:-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "RdjteKgXMTDR",
        "outputId": "a068be79-8b97-4703-b6e9-5242a1c574ae"
      },
      "source": [
        "training_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature1</th>\n",
              "      <th>feature3</th>\n",
              "      <th>feature4</th>\n",
              "      <th>feature5</th>\n",
              "      <th>feature6</th>\n",
              "      <th>feature7</th>\n",
              "      <th>feature8</th>\n",
              "      <th>feature9</th>\n",
              "      <th>feature10</th>\n",
              "      <th>feature11</th>\n",
              "      <th>feature12</th>\n",
              "      <th>feature13</th>\n",
              "      <th>feature14</th>\n",
              "      <th>feature15</th>\n",
              "      <th>feature16</th>\n",
              "      <th>feature17</th>\n",
              "      <th>feature18</th>\n",
              "      <th>feature19</th>\n",
              "      <th>feature20</th>\n",
              "      <th>feature21</th>\n",
              "      <th>feature22</th>\n",
              "      <th>feature23</th>\n",
              "      <th>feature24</th>\n",
              "      <th>feature25</th>\n",
              "      <th>feature26</th>\n",
              "      <th>feature27</th>\n",
              "      <th>feature28</th>\n",
              "      <th>feature29</th>\n",
              "      <th>feature30</th>\n",
              "      <th>feature31</th>\n",
              "      <th>feature32</th>\n",
              "      <th>feature33</th>\n",
              "      <th>feature34</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>0.347937</td>\n",
              "      <td>0.720619</td>\n",
              "      <td>0.081004</td>\n",
              "      <td>0.705346</td>\n",
              "      <td>-0.261950</td>\n",
              "      <td>0.845898</td>\n",
              "      <td>-0.243073</td>\n",
              "      <td>0.725650</td>\n",
              "      <td>-0.455875</td>\n",
              "      <td>0.929584</td>\n",
              "      <td>-0.284651</td>\n",
              "      <td>0.901754</td>\n",
              "      <td>-0.217781</td>\n",
              "      <td>0.942532</td>\n",
              "      <td>-0.191640</td>\n",
              "      <td>0.975730</td>\n",
              "      <td>0.027451</td>\n",
              "      <td>0.881622</td>\n",
              "      <td>-0.124173</td>\n",
              "      <td>1.005963</td>\n",
              "      <td>-0.061633</td>\n",
              "      <td>0.967068</td>\n",
              "      <td>0.059713</td>\n",
              "      <td>1.012195</td>\n",
              "      <td>0.179386</td>\n",
              "      <td>0.767291</td>\n",
              "      <td>0.071265</td>\n",
              "      <td>1.079303</td>\n",
              "      <td>-0.060088</td>\n",
              "      <td>1.006549</td>\n",
              "      <td>-0.060062</td>\n",
              "      <td>1.096110</td>\n",
              "      <td>-0.109345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>0.347937</td>\n",
              "      <td>0.720619</td>\n",
              "      <td>-0.434746</td>\n",
              "      <td>0.767382</td>\n",
              "      <td>-0.144765</td>\n",
              "      <td>0.048056</td>\n",
              "      <td>-0.260682</td>\n",
              "      <td>0.283774</td>\n",
              "      <td>-0.340922</td>\n",
              "      <td>0.667755</td>\n",
              "      <td>-0.379595</td>\n",
              "      <td>0.515133</td>\n",
              "      <td>-0.188763</td>\n",
              "      <td>0.527491</td>\n",
              "      <td>-0.512822</td>\n",
              "      <td>-0.299713</td>\n",
              "      <td>-0.289723</td>\n",
              "      <td>0.472946</td>\n",
              "      <td>-0.284612</td>\n",
              "      <td>0.550047</td>\n",
              "      <td>0.047271</td>\n",
              "      <td>1.055911</td>\n",
              "      <td>-0.450605</td>\n",
              "      <td>-0.146347</td>\n",
              "      <td>-0.537023</td>\n",
              "      <td>-0.034465</td>\n",
              "      <td>-0.243102</td>\n",
              "      <td>-0.087822</td>\n",
              "      <td>-0.009613</td>\n",
              "      <td>-0.129179</td>\n",
              "      <td>-0.854455</td>\n",
              "      <td>0.272520</td>\n",
              "      <td>-0.170924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>0.347937</td>\n",
              "      <td>0.513791</td>\n",
              "      <td>0.765635</td>\n",
              "      <td>0.597644</td>\n",
              "      <td>0.562294</td>\n",
              "      <td>0.390852</td>\n",
              "      <td>1.069822</td>\n",
              "      <td>-0.110376</td>\n",
              "      <td>1.236051</td>\n",
              "      <td>-0.505775</td>\n",
              "      <td>1.484728</td>\n",
              "      <td>-0.679638</td>\n",
              "      <td>1.549644</td>\n",
              "      <td>-0.842549</td>\n",
              "      <td>1.641461</td>\n",
              "      <td>-1.224796</td>\n",
              "      <td>1.369040</td>\n",
              "      <td>-1.372242</td>\n",
              "      <td>0.967035</td>\n",
              "      <td>-1.757848</td>\n",
              "      <td>0.721881</td>\n",
              "      <td>-2.025226</td>\n",
              "      <td>0.276129</td>\n",
              "      <td>-1.968680</td>\n",
              "      <td>-0.105828</td>\n",
              "      <td>-2.365129</td>\n",
              "      <td>-0.312299</td>\n",
              "      <td>-1.614757</td>\n",
              "      <td>-0.784613</td>\n",
              "      <td>-1.388831</td>\n",
              "      <td>-0.980493</td>\n",
              "      <td>-1.217081</td>\n",
              "      <td>-1.239833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.347937</td>\n",
              "      <td>0.410337</td>\n",
              "      <td>-0.294884</td>\n",
              "      <td>-1.766851</td>\n",
              "      <td>-1.999563</td>\n",
              "      <td>-1.298488</td>\n",
              "      <td>-1.312934</td>\n",
              "      <td>0.581053</td>\n",
              "      <td>-0.280221</td>\n",
              "      <td>0.743496</td>\n",
              "      <td>-0.328748</td>\n",
              "      <td>-0.941424</td>\n",
              "      <td>1.103812</td>\n",
              "      <td>-0.833833</td>\n",
              "      <td>-1.640119</td>\n",
              "      <td>0.768682</td>\n",
              "      <td>-0.221078</td>\n",
              "      <td>0.671280</td>\n",
              "      <td>-0.256736</td>\n",
              "      <td>-0.666229</td>\n",
              "      <td>0.964564</td>\n",
              "      <td>-1.165607</td>\n",
              "      <td>1.643256</td>\n",
              "      <td>0.679721</td>\n",
              "      <td>0.066878</td>\n",
              "      <td>0.323862</td>\n",
              "      <td>-0.333335</td>\n",
              "      <td>0.695268</td>\n",
              "      <td>-0.222812</td>\n",
              "      <td>-0.990499</td>\n",
              "      <td>-1.514691</td>\n",
              "      <td>-1.022271</td>\n",
              "      <td>-1.309206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>0.347937</td>\n",
              "      <td>0.720619</td>\n",
              "      <td>-2.365858</td>\n",
              "      <td>-1.156208</td>\n",
              "      <td>-0.251490</td>\n",
              "      <td>0.465469</td>\n",
              "      <td>-2.135402</td>\n",
              "      <td>0.585668</td>\n",
              "      <td>0.764956</td>\n",
              "      <td>-1.588835</td>\n",
              "      <td>-2.230258</td>\n",
              "      <td>-0.644182</td>\n",
              "      <td>-0.188763</td>\n",
              "      <td>-0.527182</td>\n",
              "      <td>-0.155185</td>\n",
              "      <td>0.547945</td>\n",
              "      <td>-1.546904</td>\n",
              "      <td>0.600224</td>\n",
              "      <td>-1.115280</td>\n",
              "      <td>-0.552115</td>\n",
              "      <td>-0.016010</td>\n",
              "      <td>-0.295901</td>\n",
              "      <td>-1.787057</td>\n",
              "      <td>-2.413575</td>\n",
              "      <td>-1.826594</td>\n",
              "      <td>-2.986491</td>\n",
              "      <td>-1.691672</td>\n",
              "      <td>1.079303</td>\n",
              "      <td>-1.913666</td>\n",
              "      <td>1.132992</td>\n",
              "      <td>-1.939751</td>\n",
              "      <td>-0.668430</td>\n",
              "      <td>-0.030918</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     feature1  feature3  feature4  ...  feature32  feature33  feature34\n",
              "270  0.347937  0.720619  0.081004  ...  -0.060062   1.096110  -0.109345\n",
              "116  0.347937  0.720619 -0.434746  ...  -0.854455   0.272520  -0.170924\n",
              "135  0.347937  0.513791  0.765635  ...  -0.980493  -1.217081  -1.239833\n",
              "91   0.347937  0.410337 -0.294884  ...  -1.514691  -1.022271  -1.309206\n",
              "100  0.347937  0.720619 -2.365858  ...  -1.939751  -0.668430  -0.030918\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUhxliusDTHZ"
      },
      "source": [
        "Checking Data before Pre-Processing & Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDT7DMu2s9zB",
        "outputId": "7c573732-eab5-4dba-c834-329385795cea"
      },
      "source": [
        "training_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(211, 33)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3KhDcCTtNdF",
        "outputId": "6b465df1-6c03-48fa-da89-cd2fb5110a8b"
      },
      "source": [
        "training_label.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(211,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZrtHkB-tJLZ",
        "outputId": "194e4f49-58f6-43dc-b359-3548b20a3381"
      },
      "source": [
        "testing_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(140, 33)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ9fGE3CtTOf",
        "outputId": "84082311-e83d-42c9-b0db-9bcfdc23f3e5"
      },
      "source": [
        "testing_label.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(140,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5nqz69OJApV",
        "outputId": "27091dc4-def8-4e83-a99d-99638d783b7b"
      },
      "source": [
        "len(training_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "211"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4N2Exphu2wf",
        "outputId": "46f727c9-dbfa-43c5-f027-6e6119908320"
      },
      "source": [
        "type (training_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzZOqsAmzrHd"
      },
      "source": [
        "training_data = np.asarray(training_data).astype('float64')\n",
        "training_label = np.asarray(training_label).astype('float32')\n",
        "\n",
        "testing_data = np.asarray(testing_data).astype('float64')\n",
        "testing_label = np.asarray(testing_label).astype('float64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCFqboT8yY_w",
        "outputId": "aa7e10f7-566e-4eee-f8be-2432e5e8502a"
      },
      "source": [
        "print(training_data.dtype) \n",
        "print(training_label.dtype) \n",
        "\n",
        "print(testing_data.dtype)\n",
        "print(testing_label.dtype) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "float64\n",
            "float32\n",
            "float64\n",
            "float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7XhyhePtodR"
      },
      "source": [
        "Training & Validation data consists of only 211 records."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3uk24ActthU"
      },
      "source": [
        "Model Requirements: 1 Hidden Layers with 16 outputs. \n",
        "\n",
        "As it is a binary classification problem the loss will be binary_crossentropy and metrics accuracy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGlIeNLktxsY"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb6YdfEqtybH"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(16, activation='relu', input_shape=(training_data.shape[1],)))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9COsvf-uqUU",
        "outputId": "c3c6f3ef-a5eb-423f-deb1-adb99782229a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_20 (Dense)             (None, 16)                544       \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 561\n",
            "Trainable params: 561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XiGJMPdu-UG"
      },
      "source": [
        "Compilation Step (Note : Its a Binary problem , select loss , metrics according to it)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhLHm4bpuuWt"
      },
      "source": [
        "model.compile(optimizer = 'RMSprop', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK49DO2xux2h",
        "outputId": "fa2f7fef-7143-4ec3-a538-12c948bac7dc"
      },
      "source": [
        "history = model.fit(training_data, training_label, validation_split=0.2, epochs=100, batch_size = 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "9/9 [==============================] - 1s 26ms/step - loss: 0.7979 - accuracy: 0.3861 - val_loss: 0.7034 - val_accuracy: 0.5581\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6943 - accuracy: 0.5404 - val_loss: 0.6263 - val_accuracy: 0.7442\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6045 - accuracy: 0.7061 - val_loss: 0.5637 - val_accuracy: 0.8372\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5966 - accuracy: 0.7026 - val_loss: 0.5156 - val_accuracy: 0.8837\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.5311 - accuracy: 0.7834 - val_loss: 0.4734 - val_accuracy: 0.8837\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.5188 - accuracy: 0.7710 - val_loss: 0.4402 - val_accuracy: 0.8837\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.4549 - accuracy: 0.8413 - val_loss: 0.4128 - val_accuracy: 0.9070\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.4618 - accuracy: 0.8186 - val_loss: 0.3885 - val_accuracy: 0.9070\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4339 - accuracy: 0.8381 - val_loss: 0.3690 - val_accuracy: 0.8837\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.4030 - accuracy: 0.8537 - val_loss: 0.3498 - val_accuracy: 0.9070\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3789 - accuracy: 0.8610 - val_loss: 0.3357 - val_accuracy: 0.9070\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3576 - accuracy: 0.8611 - val_loss: 0.3212 - val_accuracy: 0.9070\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3393 - accuracy: 0.8885 - val_loss: 0.3073 - val_accuracy: 0.9070\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3592 - accuracy: 0.8299 - val_loss: 0.2955 - val_accuracy: 0.9070\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.3493 - accuracy: 0.8641 - val_loss: 0.2847 - val_accuracy: 0.9070\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.3320 - accuracy: 0.8688 - val_loss: 0.2743 - val_accuracy: 0.9070\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.2997 - accuracy: 0.8884 - val_loss: 0.2651 - val_accuracy: 0.9070\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2709 - accuracy: 0.9079 - val_loss: 0.2557 - val_accuracy: 0.9070\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2851 - accuracy: 0.8812 - val_loss: 0.2476 - val_accuracy: 0.9302\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.2874 - accuracy: 0.8704 - val_loss: 0.2404 - val_accuracy: 0.9302\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.2751 - accuracy: 0.8924 - val_loss: 0.2334 - val_accuracy: 0.9302\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2606 - accuracy: 0.9010 - val_loss: 0.2264 - val_accuracy: 0.9302\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2428 - accuracy: 0.9178 - val_loss: 0.2192 - val_accuracy: 0.9535\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2625 - accuracy: 0.8944 - val_loss: 0.2127 - val_accuracy: 0.9535\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2296 - accuracy: 0.9273 - val_loss: 0.2084 - val_accuracy: 0.9535\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.2325 - accuracy: 0.9067 - val_loss: 0.2011 - val_accuracy: 0.9535\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.2098 - accuracy: 0.9376 - val_loss: 0.1959 - val_accuracy: 0.9535\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.2078 - accuracy: 0.9234 - val_loss: 0.1909 - val_accuracy: 0.9535\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.2168 - accuracy: 0.9261 - val_loss: 0.1860 - val_accuracy: 0.9535\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.2177 - accuracy: 0.9277 - val_loss: 0.1818 - val_accuracy: 0.9535\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1943 - accuracy: 0.9398 - val_loss: 0.1784 - val_accuracy: 0.9535\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1926 - accuracy: 0.9439 - val_loss: 0.1743 - val_accuracy: 0.9535\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1950 - accuracy: 0.9385 - val_loss: 0.1717 - val_accuracy: 0.9535\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.2098 - accuracy: 0.9314 - val_loss: 0.1693 - val_accuracy: 0.9535\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.2212 - accuracy: 0.9093 - val_loss: 0.1658 - val_accuracy: 0.9535\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1952 - accuracy: 0.9415 - val_loss: 0.1609 - val_accuracy: 0.9535\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1805 - accuracy: 0.9704 - val_loss: 0.1563 - val_accuracy: 0.9535\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1733 - accuracy: 0.9493 - val_loss: 0.1534 - val_accuracy: 0.9535\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1673 - accuracy: 0.9598 - val_loss: 0.1497 - val_accuracy: 0.9535\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1707 - accuracy: 0.9669 - val_loss: 0.1472 - val_accuracy: 0.9535\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1550 - accuracy: 0.9619 - val_loss: 0.1440 - val_accuracy: 0.9535\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1948 - accuracy: 0.9423 - val_loss: 0.1411 - val_accuracy: 0.9535\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1725 - accuracy: 0.9435 - val_loss: 0.1370 - val_accuracy: 0.9535\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1862 - accuracy: 0.9391 - val_loss: 0.1345 - val_accuracy: 0.9535\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1766 - accuracy: 0.9336 - val_loss: 0.1308 - val_accuracy: 0.9535\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1588 - accuracy: 0.9351 - val_loss: 0.1274 - val_accuracy: 0.9535\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1411 - accuracy: 0.9610 - val_loss: 0.1247 - val_accuracy: 0.9535\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1461 - accuracy: 0.9629 - val_loss: 0.1231 - val_accuracy: 0.9535\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1851 - accuracy: 0.9171 - val_loss: 0.1204 - val_accuracy: 0.9535\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1361 - accuracy: 0.9721 - val_loss: 0.1165 - val_accuracy: 0.9535\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1489 - accuracy: 0.9625 - val_loss: 0.1140 - val_accuracy: 0.9535\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1390 - accuracy: 0.9626 - val_loss: 0.1107 - val_accuracy: 0.9535\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1298 - accuracy: 0.9803 - val_loss: 0.1100 - val_accuracy: 0.9535\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1514 - accuracy: 0.9507 - val_loss: 0.1066 - val_accuracy: 0.9535\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1309 - accuracy: 0.9753 - val_loss: 0.1033 - val_accuracy: 0.9535\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1192 - accuracy: 0.9741 - val_loss: 0.1017 - val_accuracy: 0.9767\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1195 - accuracy: 0.9685 - val_loss: 0.0992 - val_accuracy: 0.9767\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1219 - accuracy: 0.9695 - val_loss: 0.0969 - val_accuracy: 0.9767\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1392 - accuracy: 0.9630 - val_loss: 0.0940 - val_accuracy: 0.9767\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1086 - accuracy: 0.9697 - val_loss: 0.0932 - val_accuracy: 0.9767\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1415 - accuracy: 0.9669 - val_loss: 0.0914 - val_accuracy: 0.9767\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1298 - accuracy: 0.9701 - val_loss: 0.0899 - val_accuracy: 0.9767\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1346 - accuracy: 0.9657 - val_loss: 0.0882 - val_accuracy: 0.9767\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1112 - accuracy: 0.9831 - val_loss: 0.0863 - val_accuracy: 0.9767\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1349 - accuracy: 0.9710 - val_loss: 0.0844 - val_accuracy: 0.9767\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1249 - accuracy: 0.9779 - val_loss: 0.0823 - val_accuracy: 0.9767\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1183 - accuracy: 0.9693 - val_loss: 0.0804 - val_accuracy: 0.9767\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0998 - accuracy: 0.9810 - val_loss: 0.0792 - val_accuracy: 0.9767\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1118 - accuracy: 0.9784 - val_loss: 0.0779 - val_accuracy: 0.9767\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1135 - accuracy: 0.9786 - val_loss: 0.0782 - val_accuracy: 0.9767\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1156 - accuracy: 0.9635 - val_loss: 0.0772 - val_accuracy: 0.9767\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1060 - accuracy: 0.9832 - val_loss: 0.0763 - val_accuracy: 0.9767\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1112 - accuracy: 0.9761 - val_loss: 0.0736 - val_accuracy: 0.9767\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1117 - accuracy: 0.9615 - val_loss: 0.0731 - val_accuracy: 0.9767\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0958 - accuracy: 0.9719 - val_loss: 0.0727 - val_accuracy: 0.9535\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0978 - accuracy: 0.9821 - val_loss: 0.0724 - val_accuracy: 0.9535\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0890 - accuracy: 0.9755 - val_loss: 0.0713 - val_accuracy: 0.9535\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1151 - accuracy: 0.9709 - val_loss: 0.0706 - val_accuracy: 0.9535\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0773 - accuracy: 0.9816 - val_loss: 0.0709 - val_accuracy: 0.9535\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0931 - accuracy: 0.9746 - val_loss: 0.0705 - val_accuracy: 0.9535\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0696 - accuracy: 0.9888 - val_loss: 0.0699 - val_accuracy: 0.9535\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0918 - accuracy: 0.9711 - val_loss: 0.0695 - val_accuracy: 0.9535\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0717 - accuracy: 0.9840 - val_loss: 0.0696 - val_accuracy: 0.9535\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0834 - accuracy: 0.9786 - val_loss: 0.0678 - val_accuracy: 0.9535\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0778 - accuracy: 0.9781 - val_loss: 0.0692 - val_accuracy: 0.9535\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0844 - accuracy: 0.9721 - val_loss: 0.0683 - val_accuracy: 0.9535\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0793 - accuracy: 0.9859 - val_loss: 0.0668 - val_accuracy: 0.9535\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0838 - accuracy: 0.9850 - val_loss: 0.0666 - val_accuracy: 0.9535\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0693 - accuracy: 0.9924 - val_loss: 0.0656 - val_accuracy: 0.9535\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0819 - accuracy: 0.9783 - val_loss: 0.0652 - val_accuracy: 0.9767\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0878 - accuracy: 0.9754 - val_loss: 0.0629 - val_accuracy: 0.9767\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0833 - accuracy: 0.9796 - val_loss: 0.0626 - val_accuracy: 0.9767\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0613 - accuracy: 0.9957 - val_loss: 0.0617 - val_accuracy: 0.9767\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0829 - accuracy: 0.9944 - val_loss: 0.0627 - val_accuracy: 0.9767\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0663 - accuracy: 0.9975 - val_loss: 0.0627 - val_accuracy: 0.9767\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0635 - accuracy: 0.9975 - val_loss: 0.0622 - val_accuracy: 0.9767\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0597 - accuracy: 0.9927 - val_loss: 0.0621 - val_accuracy: 0.9767\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0533 - accuracy: 0.9944 - val_loss: 0.0621 - val_accuracy: 0.9767\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0557 - accuracy: 0.9988 - val_loss: 0.0644 - val_accuracy: 0.9767\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0643 - accuracy: 0.9956 - val_loss: 0.0631 - val_accuracy: 0.9767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTqSZ7jNvvql",
        "outputId": "2d99be33-5f10-470a-c2da-692acb09676c"
      },
      "source": [
        "history.history.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Clf_fXqIvyz2",
        "outputId": "3d80977a-47c9-4ce7-fddc-5e86c5b77aa3"
      },
      "source": [
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(history_dict['accuracy']) + 1)\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training / validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hWOO6_6BGoCv",
        "outputId": "689401dd-fd0d-4669-c513-60d5fb731fbf"
      },
      "source": [
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
        "plt.title('Training / validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQK3wnW9voCj"
      },
      "source": [
        "Retrain Model to curb overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6sOELA6vjqX",
        "outputId": "0175b414-9131-4c2c-abbb-a9096282ee19"
      },
      "source": [
        "history = model.fit(training_data, training_label, validation_split=0.2, epochs=80, batch_size = 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.0598 - accuracy: 0.9940 - val_loss: 0.0623 - val_accuracy: 0.9767\n",
            "Epoch 2/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0587 - accuracy: 0.9940 - val_loss: 0.0606 - val_accuracy: 0.9767\n",
            "Epoch 3/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0571 - accuracy: 0.9940 - val_loss: 0.0594 - val_accuracy: 0.9767\n",
            "Epoch 4/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0562 - accuracy: 0.9940 - val_loss: 0.0587 - val_accuracy: 0.9767\n",
            "Epoch 5/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0552 - accuracy: 0.9940 - val_loss: 0.0576 - val_accuracy: 0.9767\n",
            "Epoch 6/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0537 - accuracy: 0.9940 - val_loss: 0.0581 - val_accuracy: 0.9767\n",
            "Epoch 7/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0530 - accuracy: 0.9940 - val_loss: 0.0572 - val_accuracy: 0.9767\n",
            "Epoch 8/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0519 - accuracy: 0.9940 - val_loss: 0.0556 - val_accuracy: 0.9767\n",
            "Epoch 9/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0508 - accuracy: 0.9940 - val_loss: 0.0542 - val_accuracy: 0.9767\n",
            "Epoch 10/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9940 - val_loss: 0.0558 - val_accuracy: 0.9767\n",
            "Epoch 11/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0487 - accuracy: 0.9940 - val_loss: 0.0555 - val_accuracy: 0.9767\n",
            "Epoch 12/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0479 - accuracy: 0.9940 - val_loss: 0.0562 - val_accuracy: 0.9767\n",
            "Epoch 13/80\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0464 - accuracy: 0.9940 - val_loss: 0.0537 - val_accuracy: 0.9767\n",
            "Epoch 14/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0461 - accuracy: 0.9940 - val_loss: 0.0530 - val_accuracy: 0.9767\n",
            "Epoch 15/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0449 - accuracy: 0.9940 - val_loss: 0.0534 - val_accuracy: 0.9767\n",
            "Epoch 16/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0443 - accuracy: 0.9940 - val_loss: 0.0527 - val_accuracy: 0.9767\n",
            "Epoch 17/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0433 - accuracy: 0.9940 - val_loss: 0.0525 - val_accuracy: 0.9767\n",
            "Epoch 18/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0424 - accuracy: 0.9940 - val_loss: 0.0530 - val_accuracy: 0.9767\n",
            "Epoch 19/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0415 - accuracy: 0.9940 - val_loss: 0.0522 - val_accuracy: 0.9767\n",
            "Epoch 20/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0410 - accuracy: 0.9940 - val_loss: 0.0545 - val_accuracy: 0.9767\n",
            "Epoch 21/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0400 - accuracy: 0.9940 - val_loss: 0.0548 - val_accuracy: 0.9767\n",
            "Epoch 22/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0393 - accuracy: 0.9940 - val_loss: 0.0545 - val_accuracy: 0.9767\n",
            "Epoch 23/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0384 - accuracy: 0.9940 - val_loss: 0.0534 - val_accuracy: 0.9767\n",
            "Epoch 24/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0378 - accuracy: 0.9940 - val_loss: 0.0522 - val_accuracy: 0.9767\n",
            "Epoch 25/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0370 - accuracy: 0.9940 - val_loss: 0.0549 - val_accuracy: 0.9767\n",
            "Epoch 26/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0364 - accuracy: 0.9940 - val_loss: 0.0535 - val_accuracy: 0.9767\n",
            "Epoch 27/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0357 - accuracy: 0.9940 - val_loss: 0.0550 - val_accuracy: 0.9767\n",
            "Epoch 28/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0350 - accuracy: 0.9940 - val_loss: 0.0562 - val_accuracy: 0.9767\n",
            "Epoch 29/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0340 - accuracy: 0.9940 - val_loss: 0.0554 - val_accuracy: 0.9767\n",
            "Epoch 30/80\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0334 - accuracy: 0.9940 - val_loss: 0.0544 - val_accuracy: 0.9767\n",
            "Epoch 31/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0330 - accuracy: 0.9940 - val_loss: 0.0559 - val_accuracy: 0.9767\n",
            "Epoch 32/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0325 - accuracy: 0.9940 - val_loss: 0.0555 - val_accuracy: 0.9767\n",
            "Epoch 33/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0316 - accuracy: 0.9940 - val_loss: 0.0556 - val_accuracy: 0.9767\n",
            "Epoch 34/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0312 - accuracy: 0.9940 - val_loss: 0.0559 - val_accuracy: 0.9767\n",
            "Epoch 35/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0304 - accuracy: 0.9940 - val_loss: 0.0560 - val_accuracy: 0.9767\n",
            "Epoch 36/80\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.0301 - accuracy: 0.9940 - val_loss: 0.0572 - val_accuracy: 0.9767\n",
            "Epoch 37/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0293 - accuracy: 0.9940 - val_loss: 0.0569 - val_accuracy: 0.9767\n",
            "Epoch 38/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0290 - accuracy: 0.9940 - val_loss: 0.0575 - val_accuracy: 0.9767\n",
            "Epoch 39/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0283 - accuracy: 0.9940 - val_loss: 0.0578 - val_accuracy: 0.9767\n",
            "Epoch 40/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0277 - accuracy: 0.9940 - val_loss: 0.0614 - val_accuracy: 0.9767\n",
            "Epoch 41/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0271 - accuracy: 0.9940 - val_loss: 0.0618 - val_accuracy: 0.9767\n",
            "Epoch 42/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0268 - accuracy: 0.9940 - val_loss: 0.0599 - val_accuracy: 0.9767\n",
            "Epoch 43/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0263 - accuracy: 0.9940 - val_loss: 0.0601 - val_accuracy: 0.9767\n",
            "Epoch 44/80\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0258 - accuracy: 0.9940 - val_loss: 0.0594 - val_accuracy: 0.9767\n",
            "Epoch 45/80\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0255 - accuracy: 0.9940 - val_loss: 0.0616 - val_accuracy: 0.9767\n",
            "Epoch 46/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0248 - accuracy: 0.9940 - val_loss: 0.0653 - val_accuracy: 0.9767\n",
            "Epoch 47/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0242 - accuracy: 0.9940 - val_loss: 0.0657 - val_accuracy: 0.9767\n",
            "Epoch 48/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0240 - accuracy: 0.9940 - val_loss: 0.0641 - val_accuracy: 0.9767\n",
            "Epoch 49/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0235 - accuracy: 0.9940 - val_loss: 0.0630 - val_accuracy: 0.9767\n",
            "Epoch 50/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0228 - accuracy: 0.9940 - val_loss: 0.0660 - val_accuracy: 0.9767\n",
            "Epoch 51/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0230 - accuracy: 0.9940 - val_loss: 0.0663 - val_accuracy: 0.9767\n",
            "Epoch 52/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0223 - accuracy: 0.9940 - val_loss: 0.0666 - val_accuracy: 0.9767\n",
            "Epoch 53/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0217 - accuracy: 0.9940 - val_loss: 0.0676 - val_accuracy: 0.9767\n",
            "Epoch 54/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0215 - accuracy: 0.9940 - val_loss: 0.0641 - val_accuracy: 0.9767\n",
            "Epoch 55/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0212 - accuracy: 0.9940 - val_loss: 0.0668 - val_accuracy: 0.9767\n",
            "Epoch 56/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0206 - accuracy: 0.9940 - val_loss: 0.0674 - val_accuracy: 0.9767\n",
            "Epoch 57/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0206 - accuracy: 0.9940 - val_loss: 0.0701 - val_accuracy: 0.9767\n",
            "Epoch 58/80\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0201 - accuracy: 0.9940 - val_loss: 0.0689 - val_accuracy: 0.9767\n",
            "Epoch 59/80\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0199 - accuracy: 0.9940 - val_loss: 0.0699 - val_accuracy: 0.9767\n",
            "Epoch 60/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0195 - accuracy: 0.9940 - val_loss: 0.0692 - val_accuracy: 0.9767\n",
            "Epoch 61/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0192 - accuracy: 0.9940 - val_loss: 0.0688 - val_accuracy: 0.9767\n",
            "Epoch 62/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0190 - accuracy: 0.9940 - val_loss: 0.0698 - val_accuracy: 0.9767\n",
            "Epoch 63/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0186 - accuracy: 0.9940 - val_loss: 0.0690 - val_accuracy: 0.9767\n",
            "Epoch 64/80\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0182 - accuracy: 0.9940 - val_loss: 0.0718 - val_accuracy: 0.9767\n",
            "Epoch 65/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0178 - accuracy: 0.9940 - val_loss: 0.0720 - val_accuracy: 0.9767\n",
            "Epoch 66/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0177 - accuracy: 0.9940 - val_loss: 0.0722 - val_accuracy: 0.9767\n",
            "Epoch 67/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0177 - accuracy: 0.9940 - val_loss: 0.0755 - val_accuracy: 0.9767\n",
            "Epoch 68/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0170 - accuracy: 0.9940 - val_loss: 0.0786 - val_accuracy: 0.9767\n",
            "Epoch 69/80\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.0748 - val_accuracy: 0.9767\n",
            "Epoch 70/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0166 - accuracy: 0.9940 - val_loss: 0.0763 - val_accuracy: 0.9767\n",
            "Epoch 71/80\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0165 - accuracy: 0.9940 - val_loss: 0.0778 - val_accuracy: 0.9767\n",
            "Epoch 72/80\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0161 - accuracy: 0.9940 - val_loss: 0.0756 - val_accuracy: 0.9767\n",
            "Epoch 73/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0161 - accuracy: 0.9940 - val_loss: 0.0774 - val_accuracy: 0.9767\n",
            "Epoch 74/80\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.0158 - accuracy: 0.9940 - val_loss: 0.0802 - val_accuracy: 0.9767\n",
            "Epoch 75/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0152 - accuracy: 0.9940 - val_loss: 0.0811 - val_accuracy: 0.9767\n",
            "Epoch 76/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0156 - accuracy: 0.9940 - val_loss: 0.0830 - val_accuracy: 0.9767\n",
            "Epoch 77/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0147 - accuracy: 0.9940 - val_loss: 0.0829 - val_accuracy: 0.9767\n",
            "Epoch 78/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0837 - val_accuracy: 0.9767\n",
            "Epoch 79/80\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0146 - accuracy: 0.9940 - val_loss: 0.0840 - val_accuracy: 0.9767\n",
            "Epoch 80/80\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0847 - val_accuracy: 0.9767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOwIElGPHMCC"
      },
      "source": [
        "# Evaluation & Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOjmFa5AHOjh",
        "outputId": "4ebc4a10-c013-4465-ff63-4c376a3fc43b"
      },
      "source": [
        "accuracy_score = model.evaluate(testing_data, testing_label, batch_size=10)\n",
        "print('accuracy score ', accuracy_score[1] * 100, '%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 0s 2ms/step - loss: 0.2766 - accuracy: 0.9286\n",
            "accuracy score  92.85714030265808 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IVhnBIaIGxP"
      },
      "source": [
        "pred_score = model.predict(testing_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYb26JVLIX_r",
        "outputId": "89b16f55-1b33-49cb-95fe-cc95e93faaff"
      },
      "source": [
        "pred_label = (pred_score > 0.5)\n",
        "tf.math.confusion_matrix(\n",
        "    testing_label, pred_label, num_classes=2, weights=None, dtype=tf.dtypes.int32,name=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
              "array([[47,  8],\n",
              "       [ 2, 83]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA9fpGdVIvn_",
        "outputId": "25e490bf-777d-44b3-81dc-c20c3b1a0fa7"
      },
      "source": [
        "np.count_nonzero(pred_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    }
  ]
}